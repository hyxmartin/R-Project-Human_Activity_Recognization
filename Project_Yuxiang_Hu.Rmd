---
title: "Human Activity Recognition"
author: "Yuxiang (Martin) Hu"
output:
  html_document:
    fig_height: 5
    fig_width: 5
    toc: yes
    toc_depth: 5
    theme: sandstone
  pdf_document:
    toc: yes
---

# Introduction

Human activity recognition is a growing field with numerous applications. Companies like BodyMedia and Fitbit sell personal activity monitors worn on the left arm and waist, respectively. One can imagine numerous applications for this technology. For the purposes of this project, suppose your company would like to construct a similar device. Toward that end, you have been provided a dataset containing approximately 8 hours of accelerometer data for each of 4 individuals. Records represent 3-axis acceleration measurements taken from 4 accelerometers worn on the waist, left thigh, right arm, and right ankle. Each measurement is taken over a time window of 150ms and presented in temporal order, without a time stamp. The activity of each participant is categorized into 5 classes; sitting, sitting-down, standing, standing-up, and walking. 

# Data Preparation and Summary
## Step 1 - Load Data and load libraries
```{r}
options(warn=-1)  # ignore warning
library(gplots)
library(ggplot2)
library(plyr)
library(zoo)
library(e1071)  # SVM
library(randomForest)  # Random Forest
library(nnet)  # Multinomial Logistic Classifior
library(changepoint)  # Library for change point detection

# Utility function for importing data from a csv (comma-separated values, flat table) file
import.csv <- function(filename) {
  return(read.csv(filename, dec = ",", sep = ";", header = TRUE))
}
# Utility function for exporting data to csv file
# Note that csv files are very portable, practically all tools understand them,
# including Microsoft Excel
write.csv <- function(ob, filename) {
  write.table(ob, filename, quote = FALSE, sep = ";", dec = ",", row.names = FALSE)
}

my.data <- import.csv("dataset-har-PUC-Rio-ugulino.csv")
```
## Step 2 - Clean data
Z4 was not loaded as numeric value, because a null record exists.
I will remove the single na record and convert z4 back to integer.
```{r}
# Function to remove special characters
my.data <- transform(my.data,
                     z4 = as.integer(as.character(z4))
)
summary(my.data) 
my.data <- my.data[!is.na(my.data$z4),]  # Drop Z4 na record

```
## Step 3 - Data Summary
```{r}
trim <- function(x) gsub("^\\s+|\\s+$", "", x)

# Function to determine missing symbol
is.missing.symbol <- function(x) {
  if (nchar(trim(x)) == 0) {
    missing = 1
  } else {
    missing = 0
  }
  return(missing)
}

brief <- function(df) {
  
  # those two list store two indexs, one for real and another for symbolic attributes
  real.index <- NULL
  symbol.index <- NULL
  
  num_row <- nrow(df)  # count number of rows 
  num_att <- ncol(df)  # count number of columns 
  
  for (ii in 1:num_att) {
    this.att <- df[, ii]
    if (is.numeric(this.att)) {
      real.index <- c(real.index, ii)
    }
    if (is.factor(this.att)) {
      symbol.index <- c(symbol.index, ii)
    }
  }
  
  cat("This data set has", num_row, "rows,", num_att, "attributes")  # write a line here: use cat function to print the "This data set has xxx row, xxx attributes
  
  real.out <- NULL  # this data frame store information for real valued attributes
  for (index in real.index) {
    this.att <- df[, index]  # extract a specific column
    att_name <- colnames(df)[index]  # get attribute name using colnames function
    num_missing <- sum(is.na(this.att))  # count number of missing values using is.na function 
    Min <- round(min(this.att), 2)  # min
    Max <- round(max(this.att), 2)  # max
    Mean <- round(mean(this.att), 2)  # mean
    Median <- round(median(this.att), 2)  # median
    Sdev <- round(sd(this.att), 2)  # standard deviation
    Var <- round(var(this.att), 2)  # variance
    this.line <- data.frame(Attribute_ID = index, Attribute_Name = att_name, Missing = num_missing, 
                            Min, Max, Mean, Median, Sdev, Variance = Var) # assemble into a line
    real.out <- rbind(real.out, this.line) # concatenate to get a data frame
  }
  
  cat("real valued attributes\n")
  cat("======================\n")
  print(real.out)
  
  # gather stats for symbolic attributes
  
  symbol.out <- NULL #this data frame store information for real valued attributes
  max_MCV = 5
  for (index in symbol.index) {
    this.att <- df[, index]
    att_name <- colnames(df)[index]  # get attribute name 
    
    #count number of missing values in symbolic attribute
    num_missing <- length(this.att[this.att == ''])
    non_missing_id <- which(unlist(lapply(this.att, is.missing.symbol)) == 0)        
    this.att <- this.att[non_missing_id]
    
    #count MCV
    arity <- nlevels(this.att[this.att!='',drop=TRUE]) 
    num_MCV <- min(max_MCV, arity)
    count.tbl <- as.data.frame(table(this.att))
    sorted <- count.tbl[order(-count.tbl$Freq), ][1:num_MCV, ]
    MCV_str <- ""
    for (kk in 1:nrow(sorted)) {
      MCV_value <- sorted[kk, c("this.att")]  # MCV string
      MCV_count <- sorted[kk, c("Freq")]  # MCV count
      this_str <- paste(MCV_value, "(", MCV_count, ")", sep = "")
      MCV_str <- paste(MCV_str, this_str, sep = " ")
    }
    
    
    this.line <- data.frame(Attribute_ID = index, Attribute_Name = att_name, Missing = num_missing, 
                            arity, MCVs_counts = MCV_str)
    symbol.out <- rbind(symbol.out, this.line)
  }
  cat("symbolic attributes\n")
  cat("===================\n")
  print(symbol.out)
  
}

brief(my.data)
```

# Preliminary Classification
## Step 1 - Feature Removal
The user based features are not important in this practise, because it only affects who the users are rather than defining user behaviors.
This function will remove unnessasory features from the dataset.
```{r}
feature_removal <- function(df) {
  df <- subset(df, select=-c(user, gender, age, weight, body_mass_index, how_tall_in_meters))
  return(df)
}
```
## Step 2 - Cross Validation and prediction function
In order to avoid data leakage, I will not using K fold validation process, because I would avoid same user's data present in both training and testing data.
Therefore, instead, we will choose to alternate data for each user to be testing data set. 
I will also include KNN, Naive Bayes, Random Forest, Multinomial Logistic Regression, SVM, and default classifier in our prediction model.
```{r}
do_cv <- function(df, model) {
  nn <- nrow(df)  # number of data points
  nf <- ncol(df)  # number of features
  df <- df[,c(which(colnames(df) != "class"), which(colnames(df) == "class"))]  # Move output to the last column
  folds <- split(1:nn, df[,c(which(colnames(df) == "user"))])  # user split.by column to split folds.
  score <- rep(NA, length(folds))  # create a list to hold the mse for each folds
  df <- feature_removal(df)
  for (ii in 1:length(folds)) {
    test.index <- folds[[ii]]  # extract test index 
    train.data <- df[-test.index, ]  # assemble training data
    test.data <- df[test.index, ]  # assemble test data
    if (model == 'nb') {
      fit <- naiveBayes(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'rf') {
      fit <- randomForest(formula = class ~ ., data = train.data, ntree = 100, mtry = 2, importance = TRUE)
      pred <- predict(fit, test.data)
    } else if (model == 'svm') {
      fit <- svm(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'logreg') {
      fit <- multinom(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'default') {
      pred <- rep(names(which.max(table(train.data$class))), nrow(test.data))  # Default is the majority of the ouput
    }
    accuracy <- sum(pred == test.data$class)/length(pred)
    score[ii] <- accuracy  # save the score    
  }
  return(score)
}
```
## Step 3 - Run each prediction model
### Naive Bayes
```{r}
nb_score <- do_cv(my.data, 'nb')
nb_score
```
### Random Forest
```{r}
rf_score <- do_cv(my.data, 'rf')
rf_score
```
### SVM
```{r}
svm_score <- do_cv(my.data, 'svm')
# svm_score <- do_cv(my.data, 'default')
svm_score
```
### Multinomial Logistic Regression
```{r}
logreg_score <- do_cv(my.data, 'logreg')
logreg_score
```

### Default predictor
```{r}
default_score <- do_cv(my.data, 'default')
default_score
```

### Compare among different predictor
Prepare confident data frame with mean of cross validation for each classifier, at the same time, calculate confident interval at 0.95 confidence level.
```{r}
# Prepare for plot
model.name <- c("nb_score","rf_score","svm_score", "logreg_score", "default_score")
model.mean <- c(round(mean(nb_score), 2), round(mean(rf_score),2), round(mean(svm_score), 2), round(mean(logreg_score), 2), round(mean(default_score), 2))
t.test <- t.test(nb_score, conf.level = 0.95)
ci.l <- c(t.test$conf.int[1])
ci.h <- c(t.test$conf.int[2])
t.test <- t.test(rf_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(svm_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(logreg_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(default_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
compare.scores <- data.frame(model.name, model.mean, ci.l, ci.h)
```
Plot the bar char to compare different classifier.
```{r}
# Plot barchart
bplot <- barplot2( compare.scores$model.mean,  # Data (bar heights) to plot  
          beside = TRUE,  # Plot the bars beside one another; default is to plot stacked bars  
          names.arg = compare.scores$model.name,  #Names for the bars  
          col = c("lightblue", "mistyrose", "lightcyan"),  # Color of the bars  
          border ="black",  # Color of the bar borders  
          main = c("Model comparison"),  # Main title for the plot  
          xlab = "Model",  # X-axis label  
          ylab = "Score",  # Y-axis label  
          font.lab = 2,  # Font to use for the axis labels: 1=plain text, 2=bold, 3=italic, 4=bold italic  
          plot.ci = TRUE,  # Plot confidence intervals  
          ci.l = compare.scores$ci.l,  # Lower values for the confidence interval  
          ci.u = compare.scores$ci.h,  # Upper values for the confidence interval  
          plot.grid = TRUE)  # Plot a grid  
legend(   "topright",  # Add a legend to the plot  
          legend = compare.scores$model.name,  # Text for the legend  
          fill = c("lightblue", "mistyrose", "lightcyan"),  # Fill for boxes of the legend  
          bg = "white")  # Background for legend box 
text(bplot, compare.scores$model.mean, labels = compare.scores$model.mean, pos = 3)

```
Based on the bar chart, although all classifiers are significantly better than the default score, the accuracy is still not ideal.
SVM multinomial shows the best cross validation accurcy. Next we will consider to build more features to improve the overall model performance.

# Feature Engineering
## Step 1 - Feature Extraction
The data set is consist of tri-axial accelerometers. For each accelerometer, Euler angles of roll, pitch, as well as the module (length) of the acceleartion vector can be extracted based on their tri-axial accelerometers. 
```{r}
my.data <- transform(my.data, 
                     pitch1 = with(my.data, atan2(-x1, sqrt(y1*y1 + z1*z1)) * 180 / pi),
                     roll1 = with(my.data, atan2(y1, z1) * 180 / pi),
                     module1 = with(my.data, sqrt(x1*x1 + y1*y1 + z1*z1)), 
                     pitch2 = with(my.data, atan2(-x2, sqrt(y2*y2 + z2*z2)) * 180 / pi),
                     roll2 = with(my.data, atan2(y2, z2) * 180 / pi),
                     module2 = with(my.data, sqrt(x2*x2 + y2*y2 + z2*z2)), 
                     pitch3 = with(my.data, atan2(-x3, sqrt(y3*y3 + z3*z3)) * 180 / pi),
                     roll3 = with(my.data, atan2(y3, z3) * 180 / pi),
                     module3 = with(my.data, sqrt(x3*x3 + y3*y3 + z3*z3)), 
                     pitch4 = with(my.data, atan2(-x4, sqrt(y4*y4 + z4*z4)) * 180 / pi),
                     roll4 = with(my.data, atan2(y4, z4) * 180 / pi),
                     module4 = with(my.data, sqrt(x4*x4 + y4*y4 + z4*z4))
                     )
```

## Step 2 - Feature Segmentation
Meanwhile, I will also include variance, mean, standard deviation of each raw feature and derived feature in 10 consecutive record windows. This sliding windows approach will mitigate risk of overfitting by aggregating within windows.  
To avoid reducing record size, I consider rolling 10 data points aggregation of all records without skipping any record. 
```{r}
my.data <- transform(my.data, 
                     # First sensor
                     x1.mean = with(my.data, rollapply(zoo(x1), width = 10, FUN = mean, partial = 1)),
                     y1.mean = with(my.data, rollapply(zoo(y1), width = 10, FUN = mean, partial = 1)),
                     z1.mean = with(my.data, rollapply(zoo(z1), width = 10, FUN = mean, partial = 1)),
                     pitch1.mean = with(my.data, rollapply(zoo(pitch1), width = 10, FUN = mean, partial = 1)),
                     roll1.mean = with(my.data, rollapply(zoo(roll1), width = 10, FUN = mean, partial = 1)),
                     module1.mean = with(my.data, rollapply(zoo(module1), width = 10, FUN = mean, partial = 1)),
                     x1.var = with(my.data, rollapply(zoo(x1), width = 10, FUN = var, partial = 1)),
                     y1.var = with(my.data, rollapply(zoo(y1), width = 10, FUN = var, partial = 1)),
                     z1.var = with(my.data, rollapply(zoo(z1), width = 10, FUN = var, partial = 1)),
                     pitch1.var = with(my.data, rollapply(zoo(pitch1), width = 10, FUN = var, partial = 1)),
                     roll1.var = with(my.data, rollapply(zoo(roll1), width = 10, FUN = var, partial = 1)),
                     module1.var = with(my.data, rollapply(zoo(module1), width = 10, FUN = var, partial = 1)),
                     x1.sd = with(my.data, rollapply(zoo(x1), width = 10, FUN = sd, partial = 1)),
                     y1.sd = with(my.data, rollapply(zoo(y1), width = 10, FUN = sd, partial = 1)),
                     z1.sd = with(my.data, rollapply(zoo(z1), width = 10, FUN = sd, partial = 1)),
                     pitch1.sd = with(my.data, rollapply(zoo(pitch1), width = 10, FUN = sd, partial = 1)),
                     roll1.sd = with(my.data, rollapply(zoo(roll1), width = 10, FUN = sd, partial = 1)),
                     module1.sd = with(my.data, rollapply(zoo(module1), width = 10, FUN = sd, partial = 1)),
                     
                     # Second sensor
                     x2.mean = with(my.data, rollapply(zoo(x2), width = 10, FUN = mean, partial = 1)),
                     y2.mean = with(my.data, rollapply(zoo(y2), width = 10, FUN = mean, partial = 1)),
                     z2.mean = with(my.data, rollapply(zoo(z2), width = 10, FUN = mean, partial = 1)),
                     pitch2.mean = with(my.data, rollapply(zoo(pitch2), width = 10, FUN = mean, partial = 1)),
                     roll2.mean = with(my.data, rollapply(zoo(roll2), width = 10, FUN = mean, partial = 1)),
                     module2.mean = with(my.data, rollapply(zoo(module2), width = 10, FUN = mean, partial = 1)),
                     x2.var = with(my.data, rollapply(zoo(x2), width = 10, FUN = var, partial = 1)),
                     y2.var = with(my.data, rollapply(zoo(y2), width = 10, FUN = var, partial = 1)),
                     z2.var = with(my.data, rollapply(zoo(z2), width = 10, FUN = var, partial = 1)),
                     pitch2.var = with(my.data, rollapply(zoo(pitch2), width = 10, FUN = var, partial = 1)),
                     roll2.var = with(my.data, rollapply(zoo(roll2), width = 10, FUN = var, partial = 1)),
                     module2.var = with(my.data, rollapply(zoo(module2), width = 10, FUN = var, partial = 1)),
                     x2.sd = with(my.data, rollapply(zoo(x2), width = 10, FUN = sd, partial = 1)),
                     y2.sd = with(my.data, rollapply(zoo(y2), width = 10, FUN = sd, partial = 1)),
                     z2.sd = with(my.data, rollapply(zoo(z2), width = 10, FUN = sd, partial = 1)),
                     pitch2.sd = with(my.data, rollapply(zoo(pitch2), width = 10, FUN = sd, partial = 1)),
                     roll2.sd = with(my.data, rollapply(zoo(roll2), width = 10, FUN = sd, partial = 1)),
                     module2.sd = with(my.data, rollapply(zoo(module2), width = 10, FUN = sd, partial = 1)),
                     
                     # Third sensor
                     x3.mean = with(my.data, rollapply(zoo(x3), width = 10, FUN = mean, partial = 1)),
                     y3.mean = with(my.data, rollapply(zoo(y3), width = 10, FUN = mean, partial = 1)),
                     z3.mean = with(my.data, rollapply(zoo(z3), width = 10, FUN = mean, partial = 1)),
                     pitch3.mean = with(my.data, rollapply(zoo(pitch3), width = 10, FUN = mean, partial = 1)),
                     roll3.mean = with(my.data, rollapply(zoo(roll3), width = 10, FUN = mean, partial = 1)),
                     module3.mean = with(my.data, rollapply(zoo(module3), width = 10, FUN = mean, partial = 1)),
                     x3.var = with(my.data, rollapply(zoo(x3), width = 10, FUN = var, partial = 1)),
                     y3.var = with(my.data, rollapply(zoo(y3), width = 10, FUN = var, partial = 1)),
                     z3.var = with(my.data, rollapply(zoo(z3), width = 10, FUN = var, partial = 1)),
                     pitch3.var = with(my.data, rollapply(zoo(pitch3), width = 10, FUN = var, partial = 1)),
                     roll3.var = with(my.data, rollapply(zoo(roll3), width = 10, FUN = var, partial = 1)),
                     module3.var = with(my.data, rollapply(zoo(module3), width = 10, FUN = var, partial = 1)),
                     x3.sd = with(my.data, rollapply(zoo(x3), width = 10, FUN = sd, partial = 1)),
                     y3.sd = with(my.data, rollapply(zoo(y3), width = 10, FUN = sd, partial = 1)),
                     z3.sd = with(my.data, rollapply(zoo(z3), width = 10, FUN = sd, partial = 1)),
                     pitch3.sd = with(my.data, rollapply(zoo(pitch3), width = 10, FUN = sd, partial = 1)),
                     roll3.sd = with(my.data, rollapply(zoo(roll3), width = 10, FUN = sd, partial = 1)),
                     module3.sd = with(my.data, rollapply(zoo(module3), width = 10, FUN = sd, partial = 1)),
                     
                     # Fourth sensor
                     x4.mean = with(my.data, rollapply(zoo(x4), width = 10, FUN = mean, partial = 1)),
                     y4.mean = with(my.data, rollapply(zoo(y4), width = 10, FUN = mean, partial = 1)),
                     z4.mean = with(my.data, rollapply(zoo(z4), width = 10, FUN = mean, partial = 1)),
                     pitch4.mean = with(my.data, rollapply(zoo(pitch4), width = 10, FUN = mean, partial = 1)),
                     roll4.mean = with(my.data, rollapply(zoo(roll4), width = 10, FUN = mean, partial = 1)),
                     module4.mean = with(my.data, rollapply(zoo(module4), width = 10, FUN = mean, partial = 1)),
                     x4.var = with(my.data, rollapply(zoo(x4), width = 10, FUN = var, partial = 1)),
                     y4.var = with(my.data, rollapply(zoo(y4), width = 10, FUN = var, partial = 1)),
                     z4.var = with(my.data, rollapply(zoo(z4), width = 10, FUN = var, partial = 1)),
                     pitch4.var = with(my.data, rollapply(zoo(pitch4), width = 10, FUN = var, partial = 1)),
                     roll4.var = with(my.data, rollapply(zoo(roll4), width = 10, FUN = var, partial = 1)),
                     module4.var = with(my.data, rollapply(zoo(module4), width = 10, FUN = var, partial = 1)),
                     x4.sd = with(my.data, rollapply(zoo(x4), width = 10, FUN = sd, partial = 1)),
                     y4.sd = with(my.data, rollapply(zoo(y4), width = 10, FUN = sd, partial = 1)),
                     z4.sd = with(my.data, rollapply(zoo(z4), width = 10, FUN = sd, partial = 1)),
                     pitch4.sd = with(my.data, rollapply(zoo(pitch4), width = 10, FUN = sd, partial = 1)),
                     roll4.sd = with(my.data, rollapply(zoo(roll4), width = 10, FUN = sd, partial = 1)),
                     module4.sd = with(my.data, rollapply(zoo(module4), width = 10, FUN = sd, partial = 1))
                     )
```
The data set now has 103 features, 1 output, 6 user info features, 6 raw features and 90 derived features.
```{r}
dim(my.data)
```

## Step 3 - Feature Selection
In this section, I will use PCA to select the Top principal components that all together represents more than 95% of variance.
By PCA approach, the number of features will be reduced, meanwhile, I also get rid of collinear features.
Run feature selection by prcomp
```{r}
my.newdata <- feature_removal(my.data)  #remove features with user info
my.newdata <- subset(my.newdata, select=-c(class))
pca <- prcomp(my.newdata, scale = TRUE, center = TRUE)
```
Plot the scee plot which shows the distribution of variance contained in subsequent principal components sorted by their eigenvalues.
As a result, 26 Principal Components explain about 95 % of total variance. So by using PCA, the dimensions are reduces from 96 to 26.
```{r}
screeplot(pca, npcs = 103, type = "lines")
```

```{r}
ev <- pca$sdev^2  # get eigenvalues by taking sdev squares, eigenvalues are variance in this case
# ev <- apply(pca$x, 2, var)  # alternative way to get eigenvalue
props <- ev/ sum(ev)
cumprops <- cumsum(props)
df <- data.frame(N = 1:length(cumprops), cumprops = cumprops)
min(which(df$cumprops > 0.95))
```
Create a second plot showing cumulative variance retained if top N components are kept after dimensionality reduction
```{r}
ggplot(df, aes(x = N, y = cumprops)) + 
  geom_line(colour="red", size=1) + 
  geom_point(colour="red", size=1, shape=21, fill="white") +
  xlab("N components") + ylab("Cumulative Percentage") +
  ggtitle("Top N componencts cumulative variance") +
  geom_hline(yintercept=0.95)+
  geom_vline(xintercept=26)
```

# Classification
This section is similar to the preliminary classification, upgraded with features engineering.

## Step 1 - Feature Removal
The user based features are not important in this practise, because it only affects who the users are rather than defining user behaviors.
This function will remove unnessasory features from the dataset.
```{r}
feature_removal_2 <- function(df) {
  df <- subset(df, select=-c(user))
  return(df)
}
```
## Step 2 - Cross Validation
Here I need to make a little bit improvement to include PCA features.
Also, I need to retain user for 4 folds cross validation.
```{r}
my.pca <- data.frame(user = my.data$user, pca$x[,c(1:26)], class = my.data$class)  # Retain User for 4 folds cross validation
do_cv_upgrade <- function(df, model) {
  nn <- nrow(df)  # number of data points
  nf <- ncol(df)  # number of features
  df <- df[,c(which(colnames(df) != "class"), which(colnames(df) == "class"))]  # Move output to the last column
  folds <- split(1:nn, df[,c(which(colnames(df) == "user"))])  # user split.by column to split folds.
  score <- rep(NA, length(folds))  # create a list to hold the mse for each folds
  df <- feature_removal_2(df)
  for (ii in 1:length(folds)) {
    test.index <- folds[[ii]]  # extract test index 
    train.data <- df[-test.index, ]  # assemble training data
    test.data <- df[test.index, ]  # assemble test data
    if (model == 'nb') {
      fit <- naiveBayes(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'rf') {
      fit <- randomForest(formula = class ~ ., data = train.data, ntree = 100, mtry = 2, importance = TRUE)
      pred <- predict(fit, test.data)
    } else if (model == 'svm') {
      fit <- svm(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'logreg') {
      fit <- multinom(formula = class ~ ., data = train.data)
      pred <- predict(fit, test.data)
    } else if (model == 'default') {
      pred <- rep(names(which.max(table(train.data$class))), nrow(test.data))  # Default is the majority of the ouput
    }
    accuracy <- sum(pred == test.data$class)/length(pred)
    score[ii] <- accuracy  # save the score    
  }
  return(score)
}
```


## Step 3 - Run each prediction model
### Naive Bayes
```{r}
nb_score <- do_cv_upgrade(my.pca, 'nb')
nb_score
```
### Random Forest
```{r}
rf_score <- do_cv_upgrade(my.pca, 'rf')
rf_score
```
### SVM
```{r}
svm_score <- do_cv_upgrade(my.pca, 'svm')
# svm_score <- do_cv_upgrade(my.pca, 'default')
svm_score
```
### Multinomial Logistic Regression
```{r}
logreg_score <- do_cv_upgrade(my.pca, 'logreg')
logreg_score
```
### Default predictor
```{r}
default_score <- do_cv_upgrade(my.pca, 'default')
default_score
```
### Compare among different predictor
Prepare confident data frame with mean of cross validation for each classifier, at the same time, calculate confident interval at 0.95 confidence level.
```{r}
# Prepare for plot
model.name <- c("nb_score","rf_score","svm_score", "logreg_score", "default_score")
model.mean <- c(round(mean(nb_score), 2), round(mean(rf_score),2), round(mean(svm_score), 2), round(mean(logreg_score), 2), round(mean(default_score), 2))
t.test <- t.test(nb_score, conf.level = 0.95)
ci.l <- c(t.test$conf.int[1])
ci.h <- c(t.test$conf.int[2])
t.test <- t.test(rf_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(svm_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(logreg_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(default_score, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
compare.scores <- data.frame(model.name, model.mean, ci.l, ci.h)
```
Plot the bar char to compare different classifier. After feature engineering, performance of all classifiers has been significantly enhanced. 
```{r}
# Plot barchart
bplot <- barplot2( compare.scores$model.mean,  # Data (bar heights) to plot  
          beside = TRUE,  # Plot the bars beside one another; default is to plot stacked bars  
          names.arg = compare.scores$model.name,  #Names for the bars  
          col = c("lightblue", "mistyrose", "lightcyan"),  # Color of the bars  
          border ="black",  # Color of the bar borders  
          main = c("Model comparison"),  # Main title for the plot  
          xlab = "Model",  # X-axis label  
          ylab = "Score",  # Y-axis label  
          font.lab = 2,  # Font to use for the axis labels: 1=plain text, 2=bold, 3=italic, 4=bold italic  
          plot.ci = TRUE,  # Plot confidence intervals  
          ci.l = compare.scores$ci.l,  # Lower values for the confidence interval  
          ci.u = compare.scores$ci.h,  # Upper values for the confidence interval  
          plot.grid = TRUE)  # Plot a grid  
legend(   "topright",  # Add a legend to the plot  
          legend = compare.scores$model.name,  # Text for the legend  
          fill = c("lightblue", "mistyrose", "lightcyan"),  # Fill for boxes of the legend  
          bg = "white")  # Background for legend box 
text(bplot, compare.scores$model.mean, labels = compare.scores$model.mean, pos = 3)

```

# Compare among accelerometers
Next, if only one accelerometer can be worn, I will determine which location waist, left thigh, right arm, or right ankle results in the best activity classifier in terms of both classification accuracy and change point detection. To do this, I will run with features and derived features from each accelerometers and again compare their accuracy.

## Step 1 - Select accelerometers
```{r}
acc_features <- function(df, accelerometer) {
  df <- df[,grep(accelerometer, names(df), value=TRUE)]
  return(df)
}
acc_features_1 <- data.frame(user = my.data$user, acc_features(my.data, 1), class = my.data$class)
acc_features_2 <- data.frame(user = my.data$user, acc_features(my.data, 2), class = my.data$class)
acc_features_3 <- data.frame(user = my.data$user, acc_features(my.data, 3), class = my.data$class)
acc_features_4 <- data.frame(user = my.data$user, acc_features(my.data, 4), class = my.data$class)
```

## Step 2 - Cross Validation
Since each accelerometer has 24 raw and derived features, which are of similar size to 26 PCA features. I will performance classification to all features and skip feature selection.
Again, 4 folds cross validation is adopted for measure accuracy. 
I only use one classifier to conduct this analysis. Select multinomial classification, which is one of high performance model based on previous model performance.
```{r}
logreg_score_1 <- do_cv_upgrade(acc_features_1, 'logreg')
logreg_score_2 <- do_cv_upgrade(acc_features_2, 'logreg')
logreg_score_3 <- do_cv_upgrade(acc_features_3, 'logreg')
logreg_score_4 <- do_cv_upgrade(acc_features_4, 'logreg')
```

## Step 3 - Compare accelerometers
Prepare bar plot data set
```{r}
model.name <- c("Accelerometer 1","Accelerometer 2","Accelerometer 3", "Accelerometer 4")
model.mean <- c(round(mean(logreg_score_1), 2), round(mean(logreg_score_2),2), round(mean(logreg_score_3), 2), round(mean(logreg_score_4), 2))
t.test <- t.test(logreg_score_1, conf.level = 0.95)
ci.l <- c(t.test$conf.int[1])
ci.h <- c(t.test$conf.int[2])
t.test <- t.test(logreg_score_2, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(logreg_score_3, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
t.test <- t.test(logreg_score_4, conf.level = 0.95)
ci.l <- c(ci.l, t.test$conf.int[1])
ci.h <- c(ci.h, t.test$conf.int[2])
compare.scores <- data.frame(model.name, model.mean, ci.l, ci.h)

```

Use the same bar plot with 0.95 confidence level
```{r}
# Plot barchart
bplot <- barplot2( compare.scores$model.mean,  # Data (bar heights) to plot  
          beside = TRUE,  # Plot the bars beside one another; default is to plot stacked bars  
          names.arg = compare.scores$model.name,  #Names for the bars  
          col = c("lightblue", "mistyrose", "lightcyan"),  # Color of the bars  
          border ="black",  # Color of the bar borders  
          main = c("Model comparison"),  # Main title for the plot  
          xlab = "Model",  # X-axis label  
          ylab = "Score",  # Y-axis label  
          font.lab = 2,  # Font to use for the axis labels: 1=plain text, 2=bold, 3=italic, 4=bold italic  
          plot.ci = TRUE,  # Plot confidence intervals  
          ci.l = compare.scores$ci.l,  # Lower values for the confidence interval  
          ci.u = compare.scores$ci.h,  # Upper values for the confidence interval  
          plot.grid = TRUE)  # Plot a grid  
legend(   "topright",  # Add a legend to the plot  
          legend = compare.scores$model.name,  # Text for the legend  
          fill = c("lightblue", "mistyrose", "lightcyan"),  # Fill for boxes of the legend  
          bg = "white")  # Background for legend box 
text(bplot, compare.scores$model.mean, labels = compare.scores$model.mean, pos = 3)

```

As a result, I found number 4 accelerometer on right ankle generates the best performance, with higher accuracy and smaller confident interval range.
Menawhile, the number 2 accelerometer on left thigh has about same level of mean but larger confident interval range.
In conclusion, the lower body movement determines the activities better.

# Change point detection
Change point detection can be used to detect change points by the significant changes in the temporal data. In this practise, I will use change point detection to determine how long it takes to detect a change from one type of activity to another. Additionally, I will analyze the first component of PCA result.

## Step 1 - Get a single user data
```{r}
df <- data.frame(user = my.data$user, pca1 = pca$x[,c(1)], class = my.data$class)
debora.data <- df[df$user=='debora',]

```

## Step 2 - Plot change detection and true shift activity
According to the charts plotted, the detection from sitting to sitting down and sittingdown to standing takes very short time, approximately 10 * 150ms = 1.5s.
However, the detection from standing to standing up and standingup to Walking takes around 100 * 150ms = 15s.
```{r}
par(mfrow=c(2,2))
sitting_sittingdown <- cpt.meanvar(debora.data$pca1[c(15000:16000)])
plot(sitting_sittingdown)
abline(v = c(630), col = "blue", lty = 2)
title(main = list("Sitting to Sittingdown", cex = 1.5,
                  col = "red", font = 3))

sittingdown_standing <- cpt.meanvar(debora.data$pca1[c(19000:20000)])
plot(sittingdown_standing)
abline(v = c(164), col = "blue", lty = 2)
title(main = list("Sittingdown to standing", cex = 1.5,
                  col = "red", font = 3))


standing_standingup <- cpt.meanvar(debora.data$pca1[c(33500:34500)])
plot(sittingdown_standing)
abline(v = c(604), col = "blue", lty = 2)
title(main = list("Standing to Standingup", cex = 1.5,
                  col = "red", font = 3))

standingup_walking <- cpt.meanvar(debora.data$pca1[c(37500:38500)])
plot(sittingdown_standing)
abline(v = c(457), col = "blue", lty = 2)
title(main = list("Standingup to Walking", cex = 1.5,
                  col = "red", font = 3))
```

# Summary

Our data samples only contains four individuals activities across 8 hours. Insufficient data point and user sample may lead into a biased conclusion. To have a more accurate model and reduce underfitting, more individuals and longer time traking will be required.
  The raw features along couldn't not provide a accurate modle, instead, feature Engineering brought significant improvement on classification accuracy over raw features, so I consider this as a effective approach. The final classifier with feature engineering can be used to predict acitivies very well. Meanwhile, I also found the best positions to have accelerator are right ankle and left thigh, which provide equal accuracy when using along.
  In terms of change point detection, sitting down and standing can be detected fairly quickly, whereas, standingup and walking may take up to 15s to detect. 
In conclusion, I can't make concrete conclusion based on this practise. To have an accurate model and draw a proper conclusion, I will need more test subjects and longer activities tracking time. But the takeaway from this project is that I was able to build a foundation of validation, features engineering, change point detection with a couple of prediction models such as Random Forest, Multinomial SVM, Multinomial Logistic Regression as well as Naive Baynes. 
